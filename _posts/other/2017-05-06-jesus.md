---
layout: slide
title: Model testing and selection&#58; Old and new approaches
description:
theme: solarized
transition: slide
author_profile: false
categories: presentation
---

<!-- Customisations on the theme -->
<style type="text/css">
    .reveal .slides {
      margin-top: -.3em;
      text-align: left; }
    .reveal {
      font-family: "Lato", "League Gothic", Impact, sans-serif;
      font-size: 28px; }
    .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 {
      margin: 0 0 0.25em 0;
      font-weight: bold;
      text-transform: none;
      text-shadow: 2px 2px 8px #ccc;
      font-family: "Lato", "League Gothic", Impact, sans-serif;}
    .reveal h1 {
      font-size: 1.8em; }
    .reveal h2 {
      font-size: 1.4em; }
    .reveal h3 {
      font-size: 1.25em; }
    .reveal h4 {
      font-size: 1.em; }
    .reveal .slides img {
      background: rgba(255, 255, 255, 0);
      border: none;
      box-shadow: none; }
    .reveal .slides .reference {
      text-align: right; }
    .reveal .slides .reference a {
      font-size: 80%;
      color: #080; }
    .reveal .slides blockquote {
      width: 100%;
    }
</style>



<section data-markdown>

<!-- # centring title slide only! >
<!-- .element: style="text-align: center" -->

<br>

# Model testing and selection:

## Old and new approaches

<br>

### 6$^\mathrm{th}$ of March 2017

<br>

# Jes√∫s Torrado

#### (but not my work: review talk &ndash; see references)

<br>
$\Longrightarrow$ <a href=./model_selection.pdf><font size=6>get PDF slides</font></a> $\Longleftarrow$
<br>


</section>

<section data-markdown>

## TOC

<br>
<div style="font-size: 130%">

- Introduction: the discovery of the Higgs
- Jeffreys-Lindley paradox: Freq vs Bayes vs Truth
- Beyond the Bayes ratio

<p>
... and short reviews of some statistics concepts
</p>
</div>

</section>

<section data-markdown>

## REVIEW: $p$-values and hypothesis testing

Under some hypothesis $\mathcal{H}$, the data points are distributed under some pdf

$$
x \sim \mathcal{P}\_\mathcal{H}
$$

We define the $p$-value of a (series of) event(s) $X$ under $\mathcal{H}$ as the likelihood under $\mathcal{H}$ of events *at least as extreme as* $X$:

$$
p(X) := \int\_{x\,">"\,X}\mathcal{P}\_\mathcal{H}(x)\,\mathrm{d}x
$$

We agree on rejecting $\mathcal{H}$ if we observe events so *extreme* than their $p$-value is smaller than some agreed-upon threshold $\alpha$:

$\;p(X)<\alpha\;\Longrightarrow\;$ we reject $\mathcal{H}$ with confidence $\,1-\alpha$
<!-- .element: style="text-align: center" -->

This is different from:
* probability that the tested hypothesis is false
* probability that an alternative hypothesis is true

</section>

<section data-markdown>

## REVIEW: $p$-values and hypothesis testing


![](img/p-value.svg)
<!-- .element: style="horizontal-align: center; width: 70%" -->

<!-- .element: style="text-align: center;" -->

<div class="reference" style="margin: -2.5em 0 1.5em 0;">
    <a href="https://en.wikipedia.org/wiki/P-value"> [Image source: Wikipedia]</a>
</div>

$\sigma$ | $2$    | $3$     | $4$             | $5$
---------|--------|---------|-----------------|----------------
$\alpha$ | $0.05$ | $0.003$ | $5\cdot10^{-5}$ | $5\cdot10^{-7}$

</section>

<section data-markdown>

## REVIEW: *Look-elsewhere* effect (from now on, "LEE")

Imagine a bag with maaaaany coloured balls. Let's agree on $\alpha=1/1000$.

<div style="margin: -0.5em 0 0 0;">
<div style="display:inline-block;vertical-align:center;">
    <img src="img/look_elsewhere_bag.svg" alt="img" style="width:8em"/>
</div>
<div style="display:inline-block;">
<p>
The probability of extracting a red ball is 1/1000.
</p>
<p>
Thus, if we extract a red ball, we reject the hypothesis.
</p>
<p>
But what if we extract 100 times?
</p>
<p>
The chances of getting the a red ball at all are 10%!
</p>
<p>
It would not make sense to reject the hypothesis!!!
</p>
</div></div>

+ Either we lower our threshold $\alpha$ by the number of tries,<br>
  and keep "finding the red ball" as our decisive event.<br>
  (*What particle physicists tend to do*)

+ We keep the threshold, but change the decisive event to<br>
  "finding in $N$ tries $n$ red balls such that $\mathcal{P}(n)\le \alpha$".<br>
  (*What we should do*)


<!-- OTHER STUFF:

$p$-hacking and subjectivity

ALSO: p-hacking!!! & its subjectivity: https://en.wikipedia.org/wiki/Data_dredging

http://www.statsdirect.co.uk/help/basics/p_values.htm

https://en.wikipedia.org/wiki/P-value

https://en.wikipedia.org/wiki/Misunderstandings_of_p-values
-->

</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

**DEC 2011:** CERN reports a ** *local* 4-$\sigma$** indication of a scalar particle<br>
with a mass of $127\,\mathrm{GeV}$, combining ATLAS and CMS' results.

*That could be the SM Higgs!!!*

Let's decipher that statement:

+ they measured the occurrence of 2-photon and 4-lepton production,<br>
  possible outcomes of instantaneous (*effective*) Higgs production+decay.
+ assuming a Higgs-less particle model (the *null-hypothesis*), they computed
  the probability$^\star$ distribution for the number of such events over the running time of the experiment.
  ($\star$ quantum decays are random!)
+ with that probability distribution, they found the measured excess to fall at 4-sigma *locally*, i.e.
  $p\sim 10^{-5}$ chance of such an excess (or a higher one) occurring by chance *at any energy*.


</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### *Experimentalists:*
<!-- .element style="margin: -0.8em 0 0.5em 0" -->

* In the interval of energies explored, $\sim110$&ndash;$600\,\mathrm{GeV}$, there are effectively 80 *energy bins*.
* We could have detected an excess in any of those bins, thus we need to apply the *look-elsewhere effect*, and divide the the likelihood by the number of bins.

Thus, though the *local* (i.e. pre-LEE) $p$-value is $\sim 10^{-5}$, <br>
the *global* one (i.e. post-LEE) is $\sim 10^{-3}$ ($\sim$ 3-$\sigma$).

Interesting, but could as well be a statistical fluctuation: historically, we have seen many "discoveries" of this size disappear after more data is gathered.

Thus, by **convention**, discovery: **global** $\alpha \gg$ 3-$\sigma$ (**subjective**, but reasonable), which usually means a **local** $p$-value $\ge$ 5-$\sigma$.

<div>
**NB:** *Experimentalists* and *theorists* here refer to philosophical positions: actual experimentalists and theorists may hold *each others*' position to some degree.
</div>
<!-- .element style="font-size: 60%; margins: 0 0 0 0;" -->


</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### *Theorists:*
<!-- .element style="margin: -0.8em 0 0 0" -->

Well, actually we know something about the expected **SM** Higgs mass from previous, independent experiments (LEP, Tevatron and *early* LHC):

+ we haven't seen its **direct** production $\Longrightarrow$ lower limit at $115\,\mathrm{GeV}$
+ we haven't seen its **virtual**$^\star$ production $\Longrightarrow$ upper limit of $141\,\mathrm{GeV}$
  <br>($\star$ quantum loop corrections to cross-sections)

If we had found an excess outside $[115,141]\,\mathrm{GeV}$, we would *not* have claimed that it was an **SM** Higgs (maybe some composite, SUSY, etc).

Thus, by the same argument, we should take advantage of the prior in a *positive* way: a smaller LEE accounting for that interval only. It's only fair!

Thus, the effective LEE interval should be about 1/3 of the explored one:<br>
after the effect of the LEE, $p\sim 10^{-4}$, clearly smaller than **global** 3-$\sigma$: $p\sim 10^{-3}$, possibly sufficient to call it a detection.


</section>

<section data-markdown>
<script type="text/template">
## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### *Experimentalists:*
<!-- .element style="margin: -0.8em 0 0 0" -->

Theoretical *prejudices* about the existence of something should not enter the analysis for the discovery!

All possible signals of new physics in the explored range of energies are equally likely, thus, we should apply to all of them the same criterion:<br>

**global** $\gg3$-$\sigma$ with **full** LEE.

<br>
### *Theorists:*  <!-- .element: class="fragment" -->

Actually, **no**, not all models should be treated equally!  <!-- .element: class="fragment" -->
</script>
</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### *Theorists:*
<!-- .element style="margin: -0.8em 0 0 0" -->

Why? What we are observing at the LHC is very similar to its formulation in terms of an model of particles with gauge interactions... Except for the fact that the weak bosons are **massive** and 2 of them have EM charge. This could be the result of a spontaneously broken *electro-weak* gauge theory, U(1)$\times$SU(2), as long as the corresponding scalar, gauge-singlet *Higgs boson* was found with a mass close to the $\mathcal{O}(m_\mathrm{Z,\,B})$.

This one is a very particular model: it provides a satisfactory explanation for some features of already observed data, and makes a rather constrained prediction. We can assign it a high probability within all possible extensions.

**CAREFUL:**
This does not mean that we assume that the Higgs (SM) model has a high **absolute** probability, but that it has a high **relative** probability: we are assuming that the Higgs is the most likely model for new scalars showing up *within that range of energies* in those kind of decays.


</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
    </div>

### So, what do you think?
<!-- .element style="margin: -0.7em 0 0 0" -->

Given that the result is still consistent with the **global** $\alpha \ll p(3$-$\sigma)$ criterion,
should we consider the existence of the Higgs proved at *that* point?

![](img/ATLAS_gammagamma.png)
<!-- .element style="width: 50%;" -->

<!-- .element: style="text-align: center;" -->


</section>

 <section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="http://www.science20.com/a_quantum_diaries_survivor/true_and_false_discoveries_how_to_tell_them_apart-141024"> [*True And False Discoveries: How To Tell Them Apart*, Dorigo '14]</a>
    </div>

### Historical perspective
<!-- .element style="margin: -0.7em 0 0 0" -->

![](img/oddsigma_table.jpg)
<!-- .element style="width: 50%; margin: 0em 0; horizontal-align: center" -->

<!-- .element: style="text-align: center;" -->

BICEP's $r$ $\rightarrow 6$ (False)
<!-- .element: style="text-align: center;" -->

### Do you see a pattern?
<div class="fragment">
Signals announced with an **odd**-$\sigma$ significance are <font color=#3d8><b>TRUE</b></font>!!! ;-)
</div>


</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### Frequentism vs Bayes
<!-- .element style="margin: -0.7em 0 0 0" -->

The experimentalist is a **frequentist**: he aims to carry out the analysis just in terms of the *null-hypothesis*, ignoring all *prior information* on possible alternatives. Her analysis es based on the *frequentist* probability of data in the framework of a hypothesis ($\sim$ the Bayesian likelihood). Then she decides in terms of a $p$-value how safe is to discard a hypothesis (*type I error*).

The theorist *leans* towards the **bayesian** side: the search itself is motivated by our prior knowledge (why we search in that range of energies and look at those processes!), and our hypothesis of a Higgs-ful SM is reasonable and based on previous well checked physics and collected empirical data, so why not use it when estimating the **global** $p$-value?

The experimentalist distrusts the theorist's **subjectivity**.


</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

<!-- .element style="margin: 0 0 -1.5em 0">

<!-- ### In favour of the bayesian approach -->

<div>
## Subjectivity

+ **Not so subjective:** Assessment of the prior for the same theory by different scientists may differ by a few GeV's, and that should not influence the result if the data is decisive enough. (One can carry out a *prior sensitivity analysis*.)

+ **Not so dangerous:** If the signal is physical and there is enough data, the result depends little on the prior. We will still be looking at this regime, the consequences of the decision will be further explored, at some point satisfying the experimentalists criterion. So don't fear, as long as you *account for the subjectivity*.

+ **Subjectivity is inescapable: let's account for it explicitly!** You *cannot* isolate yourselves *completely* from the assumptions of the new theory: why we have searched that particular interval, and that particular kind of interactions actually has an effect on the LEE. Also, sometimes $p$-values depend on parametrisation $\equiv$ prior (not always!)

## Model selection

The experimentalist here is just discarding a hypothesis: which one do you take among the ones that cannot be rejected? Bayes has a consistent framework for *deciding* among models.

<div>
**NB:** Neither approach can answer about the *absolute* probability of a model.
</div>
<!-- .element style="font-size: 100%; margins: 0 0 0 0;" -->

## Different answers... but to different questions!

+ **Frequentists:** is there a believable deviation from the null hypothesis within the studied range?
+ **Bayesians:** given the data, how much *more* likely is it that this is a signal of the Higgs, and not a SM statistical fluke?

</div>
<!-- .element style="font-size: 65%; margins: 0 0 0 0;" -->

</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### The question
<!-- .element style="margin: -0.7em 0 0 0" -->

> Should we specify discovery limits individually for specific experimental searches based on knowledge about and trust in the theory to be tested in the given cases?

### How to settle it?

<div class="fragment">
<br>
**Just wait!**
<p>
Gather more data until (if the signal is physical) the global $p$-value rises to discovery levels... or disappears completely.
</p><p>
In the case of the Higgs, 7 months were enough (July 2012).
</p></div>

</section>

<section data-markdown>

## **Introduction:** the discovery of the Higgs

<div class="reference">
    <a href="https://dx.doi.org/10.1086/679179"> [*Higgs discovery and the look elsewhere effect*, Dawid '13]</a>
</div>

### Time frames!
<!-- .element style="margin: -0.7em 0 0 0" -->

+ In fundamental physics today, including HEP and Cosmology, most experiments have a long time frame: years and even significant fractions of researchers' whole careers.

+ Not every hypothesis is as direct to test and trusted as the SM Higgs: discovery may need to be based not on one phenomenon, but on a complex combination of several of them, each one being inconclusive by itself: $f_\text{NL}$, CMB features/asymmetry, early/late universe, SUSY, strings...

If we *"isolate ourselves from theoretical prejudices"* too much, sticking to the experimentalist/frequentist approach, we are stuck for years!

Maybe better to take a Bayesian approach, being open about *prejudices*?

<div>
**NB:** Careful with models created *a posteriori*.
</div>
<!-- .element style="font-size: 100%; margins: 0 0 0 0;" -->

</section>

<section data-markdown>

## REVIEW: Bayes Theorem

$$
\mathcal{P}(\theta\,|\,\mathcal{D},\,\mathcal{M}) =
\frac{\mathcal{L}(\mathcal{D}\,|\,\mathcal{M}(\theta))\,\pi(\theta\,|\,\mathcal{M})}
     {\mathcal{P}(\mathcal{D}\,|\,\mathcal{M})}
$$

Terms & concepts:

+ $\mathcal{M},\,\theta$: model and its parameters

+ $\mathcal{D}$: data

+ $\pi(\theta\,|\,\mathcal{M})$: prior

+ $\mathcal{L}(\mathcal{D}\,|\,\mathcal{M}(\theta))$: likelihood

+ $\mathcal{P}(\theta\,|\,\mathcal{D},\,\mathcal{M})$: posterior

+ $\mathcal{P}(\mathcal{D}\,|\,\mathcal{M})$ or $\mathcal{Z}$: evidence, or *marginal likelihood*:
  $$
  \mathcal{P}(\mathcal{D}\,|\,\mathcal{M}) = \int
  \mathcal{L}(\mathcal{D}\,|\,\mathcal{M}(\theta))\,
  \pi(\theta\,|\,\mathcal{M})\,\mathrm{d}\theta
  $$


</section>

<section data-markdown>

## REVIEW: Bayes ratio

**Model selection**: ratio of posterior for models $\mathcal{M}_1$ and $\mathcal{M}_2$, given $\mathcal{D}$:

$$
B_{1,2} = \frac{\mathcal{P}(\mathcal{M}_1\,|\,\mathcal{D})}
               {\mathcal{P}(\mathcal{M}_2\,|\,\mathcal{D})}
        = \frac{\mathcal{P}(\mathcal{D}\,|\,\mathcal{M}_1)\,\pi(\mathcal{M}_1)}
               {\mathcal{P}(\mathcal{D}\,|\,\mathcal{M}_2)\,\pi(\mathcal{M}_2)}
        = \frac{\mathcal{Z}_1\,\pi(\mathcal{M}_1)}
               {\mathcal{Z}_2\,\pi(\mathcal{M}_2)}
$$

$$
\mathcal{Z} = \int
  \mathcal{L}(\mathcal{D}\,|\,\mathcal{M}(\theta))\,
  \pi(\theta\,|\,\mathcal{M})\,\mathrm{d}\theta
$$

### More evidence if

+ **Goodness of fit:**

$$\mathcal{Z} \leftarrow \mathcal{L}(\mathcal{D}\,|\,\mathcal{M}(\theta))$$

+ **Predictivity of the model:**

$$\mathcal{Z} \leftarrow \pi(\theta\,|\,\mathcal{M}) \sim 1/\Omega_{\mathcal{M}}$$

The (classic) frequentist approach ignores **predictivity**!

</section>

 <section data-markdown>

## REVIEW: Priors

* Prefer weakly-informative priors with reference (Jeffrey's density) and theory limits

* Improper priors are forbidden for classic model selection, but are allowed for parameter constraints (the posterior must be normalised, but not the prior).

* There are weird things people do to bypass the complication of improper priors
(e.g. <a href="https://arxiv.org/abs/1503.03414"> Mortlock '15 </a>)

</section>

<section data-markdown>

<br><br><br><br><br>
> *If both frequentist and bayesian are valid approaches to model/hypothesis testing,
they should agree, at least asymptotically (i.e. with more and more data).*

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

Imagine a process producing independent events $x$ that follow

$$ x \sim \mathcal{N}(\mu_\mathrm{true}, \sigma^2) $$

$\sigma$ is **known**, but $\mu_\mathrm{true}$ is **unknown**: we want to guess it.

Following the Central Limit Theorem, the *mean* of the events follows

$$
\bar{x} = \frac{1}{N} \sum\_{i=1}^N x\_i
\sim \mathcal{N}\left(\mu\_\mathrm{true}, \frac{\sigma^2}{N}\right)
$$

#### Hypotheses/models under consideration

+ **Hypothesis $\mathcal{H}_0$:** $\mu = \mu_\mathrm{guess}$

+ **Hypothesis $\mathcal{H}_1$:** $\mu \sim \mathrm{N}(\mu_\mathrm{guess}, \sigma^2)$


</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### The frequentist approach
<!-- .element style="margin: -0.7em 0 0 0" -->

#### Let's try to reject $\mathcal{H}\_0$!
<!-- .element style="margin: 0.7em 0 0 0" -->

Under $\mathcal{H}\_0$, $\mu = \mu\_\mathrm{guess}$, so we expect

$$
\bar{x} \sim \mathcal{N}\left(\mu\_\mathrm{guess}, \frac{\sigma^2}{N}\right)
$$

We set a rejection threshold e.g. $\alpha=0.001$

<div>
**Remember:** $\alpha$ is the probability of rejecting $\mathcal{H}\_0$ when it's true (type I error).
</div>
<!-- .element style="font-size: 70%" -->

If we draw some data and find that the $p$-value of $\bar{x}$ is smaller than $\alpha$<br>
(i.e. it is a very extreme outcome w.r.t. $\mathcal{H}\_0$), then we reject $\mathcal{H}\_0$:<br> $\mu_\mathrm{guess}$ is not a good guess for the original distribution,<br>
and we are $(1-\alpha)$% sure that it is not the correct one.

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### The bayesian approach
<!-- .element style="margin: -0.7em 0 0 0" -->

#### Let's compare $\mathcal{H}\_0$ with $\mathcal{H}\_1!$
<!-- .element style="margin: 0.7em 0 .5em 0" -->

$$\pi(\mu\,|\,\mathcal{H}\_0) = \delta(\mu-\mu\_\mathrm{guess})
\qquad\text{and}\qquad
\pi(\mu\,|\,\mathcal{H}\_1) = \mathrm{N}(\mu\_\mathrm{guess}, \sigma^2)
$$<!-- .element style="font-size: 90%" -->

<div>
**Why this prior for $\mathcal{H}_1$?** The known $\sigma$  is the only scale we know for the problem. <br>
We could also have taken a uniform prior, but we would get a similar result.
</div>
<!-- .element style="font-size: 70%" -->

* **Likelihood:**
$$
\mathcal{L}(\bar{x}\,|\,\mu) = \mathcal{N}\left(\mu, \frac{\sigma^2}{N}\right)
\quad\text{or}\quad
\mathcal{L}(t_N\,|\,\mu) = \mathcal{N}\left(0, 1\right)
\quad\text{with}\quad
t_N = \frac{\bar{x}-\mu}{\sigma/\sqrt{N}}
$$<!-- .element style="font-size: 90%" -->

<!--
* **Evidences:**
$$\mathcal{Z}\_0 = \exp\left(-\frac{1}{2} t\_N^2\right)
\qquad\text{and}\qquad
\mathcal{Z}\_1 = (N+1)^{-\frac{1}{2}} \exp\left(\frac{-1}{2(N+1)} t\_N^2\right)$$
-->

* **Bayes ratio:**

$$\mathcal{B}_{1,0} = \frac{\mathcal{Z}_1}{\mathcal{Z}_0}
   \frac{\pi(\mathcal{H}\_1)}{\pi(\mathcal{H}\_0)} =
   (N+1)^{-\frac{1}{2}} \exp\left(\frac{1}{2} t_N^2 \frac{N}{N+1}\right)
   \frac{\pi(\mathcal{H}\_1)}{\pi(\mathcal{H}\_0)}
$$<!-- .element style="font-size: 90%" -->

If $\mathcal{B}_{1,0} > 1$, we reject $\mathcal{H}\_0$ in favour of $\mathcal{H}\_1$

(Assume $\pi(\mathcal{H}\_0)=\pi(\mathcal{H}\_1)$)


</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### We draw $t_N=3+\epsilon$
<!-- .element style="margin: 0em 0 0 0" -->

### $N = 1$
<!-- .element style="margin: 1em 0 0 0" -->

![](img/jl_paradox_1.svg)
<!-- .element style="width:70%; margin: -6.5em 0 -1.5em 0" -->

<!-- .element style="text-align: center;" -->

* **Frequentist:** $\alpha < 0.001 \;\Longrightarrow\;$ <font color="#f00">**Reject $\mathcal{H}\_0$**</font>

* **Bayesian:** $\mathcal{B}_{1,0} = 6.7 > 1\;\Longrightarrow\;$ <font color="#f00">**Prefer $\mathcal{H}\_1$**</font>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### We draw $t_N=3+\epsilon$
<!-- .element style="margin: 0em 0 0 0" -->

### $N = 10$
<!-- .element style="margin: 1em 0 0 0" -->

![](img/jl_paradox_10.svg)
<!-- .element style="width:70%; margin: -6.5em 0 -1.5em 0" -->

<!-- .element style="text-align: center;" -->

* **Frequentist:** $\alpha < 0.001 \;\Longrightarrow\;$ <font color="#f00">**Reject $\mathcal{H}\_0$**</font>

* **Bayesian:** $\mathcal{B}_{1,0} = 18> 1\;\Longrightarrow\;$ <font color="#f00">**Prefer $\mathcal{H}\_1$**</font>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### We draw $t_N=3+\epsilon$
<!-- .element style="margin: 0em 0 0 0" -->

### $N = 100$
<!-- .element style="margin: 1em 0 0 0" -->

![](img/jl_paradox_100.svg)
<!-- .element style="width:70%; margin: -6.5em 0 -1.5em 0" -->

<!-- .element style="text-align: center;" -->

* **Frequentist:** $\alpha < 0.001 \;\Longrightarrow\;$ <font color="#f00">**Reject $\mathcal{H}\_0$**</font>

* **Bayesian:** $\mathcal{B}_{1,0} = 8.6 > 1\;\Longrightarrow\;$ <font color="#f00">**Prefer $\mathcal{H}\_1$**</font>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### We draw $t_N=3+\epsilon$
<!-- .element style="margin: 0em 0 0 0" -->

### $N = 1000$
<!-- .element style="margin: 1em 0 0 0" -->

![](img/jl_paradox_1000.svg)
<!-- .element style="width:70%; margin: -6.5em 0 -1.5em 0" -->

<!-- .element style="text-align: center;" -->

* **Frequentist:** $\alpha < 0.001 \;\Longrightarrow\;$ <font color="#f00">**Reject $\mathcal{H}\_0$**</font>

* **Bayesian:** $\mathcal{B}_{1,0} = 2.8 > 1\;\Longrightarrow\;$ <font color="#f00">**Prefer $\mathcal{H}\_1$**</font>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### We draw $t_N=3+\epsilon$
<!-- .element style="margin: 0em 0 0 0" -->

### $N = 10000$
<!-- .element style="margin: 1em 0 0 0" -->

![](img/jl_paradox_10000.svg)
<!-- .element style="width:70%; margin: -6.5em 0 -1.5em 0" -->

<!-- .element style="text-align: center;" -->

* **Frequentist:** $\alpha < 0.001 \;\Longrightarrow\;$ <font color="#f00">**Reject $\mathcal{H}\_0$**</font>

* **Bayesian:** $\mathcal{B}_{1,0} = 0.9 < 1\;\Longrightarrow\;$ <font color="#0f0">**Prefer $\mathcal{H}\_0$**</font>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

#### Frequentist approach:
<!-- .element style="margin: -.7em 0 0 0" -->

For a **fixed** $p$-value (here equiv. to $3+\epsilon$), it does not matter how many data we have drawn for getting it: rejection (or acceptance) only depends on the $p$-value itself.

#### Bayesian approach:

Look again at the series of images, but now mind that the grey line is actually the prior on $\mu$ under $\mathcal{H}\_1$ [c'mon, I will wait...]

As we draw more data, the *likelihood* under $\mathcal{H}\_0$ (cyan line), gets thinner and thinner, which means that the data contains more and more information.

W.r.t. to it, the prior of $\mathcal{H}\_1$ (grey line) gets wider and wider, which means that the more data we gather, the less predictive $\mathcal{H}\_1$ is.

At some point, even if $\bar{x}$ is not good at all ($p$-value $< 0.001$!), a bad guess ($\mathcal{H}\_0$) becomes better than knowing nothing at all ($\mathcal{H}\_1$)!

<div style="font-size: 60%; margin-top: -.5em;">
**NB:** This is true for almost any choice of the prior of $\mu$ in $\mathcal{H}\_1$, as long as it is proper and non-zero at $\mu_\mathrm{guess}$.
</div>

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

### A paradox!?
<!-- .element style="margin: -.7em 0 0 0" -->

The apparently *dangerous* part of this result is not just that they disagree, but that they do so *asymptotically*: the more so the more data we gather.

To many people, this means that one or the other approach is wrong, mostly since they are believed to answer which one is the **true** model behind the data in an absolute sense (they don't!).

But **F** and **B** approaches are fundamentally different: **F**'s rejection is absolute, whereas **B**'s is relative to an alternative hypothesis.

Also, they agree asymptotically on the **negative case**: if $\mathcal{H}\_0$ is not true, the more data we gather, the more clearly centred will $\bar{x}$ be away from $\mu\_\mathrm{guess}$, so it will have diminishing probability under $\mathcal{N}(\mu\_\mathrm{guess},\sigma^2/N)$, and both the **F** and **B** approaches will reject $\mathcal{H}\_0$.

There is not necessarily a paradox! ... But it tells us something about the shortcomings of *both* methods.

</section>

<section data-markdown>

## The Jeffreys-Lindley paradox

<div class="reference">
    <a href="https://arxiv.org/abs/1303.5973v3"> [*On the Jeffreys-Lindley's paradox*, Robert '13]</a>
</div>

#### Frequentist:
<!-- .element style="margin: -.7em 0 0 0" -->

Frequentists ignore the amount of information contained in the data.

But $p = 0.05$ does not mean the same in samples of different size:<br>
extreme outcomes for an statistic are more likely in small samples!<br>
(e.g. councils with highest and lowest cancer death rates are typically *small*!)

So, in order to take into account the amount of information in the data,<br>
it would be a good idea to calibrate the required $\alpha$ in terms of the size of the sample
(see e.g. <a href="http://projecteuclid.org/euclid.ejs/1464710240">Naaman '16</a>).

But, oh, the irony! This calibration is **subjective**!


</section>

<section data-markdown>

## (Some) disadvantages of the Bayes ratio

<div class="reference" style="margin-bottom: -.7em">
    <a href="https://arxiv.org/abs/1412.2044"> [*Testing hypotheses via a mixture estimation model*, Kamary et al '14]</a>
</div>

* *Model* priors matter, and they have an impact on the decision.
  <div style="font-size: 70%; margin-top: -1em">
  Though it is natural to assign the same probability to *mutually exclusive* models, it is not so clear whether it makes sense to do so for *embedded models* (e.g. in the JL paradox!).
  </div>

* *Parameter* priors matter, and have an impact on the decision.
  <div style="font-size: 70%; margin-top: -1em">
  Though the more decisive the data is, the smaller the impact (but data is not always that good).
  </div>

* Improper priors, i.e. those with infinite probability mass, are forbidden: they make the Bayes ratio mathematically not well-defined.
  <div style="font-size: 70%; margin-top: -1em">
  In the JL case, the prior of $\mathcal{H}_1$ is asymptotically improper: dominated by its diffuseness.
  </div>

* The decision rule, $\mathcal{B} <\text{or}> 1$, does not have an associated *uncertainty*, neither a natural *loss function* (though the *Jeffreys' rule* is often used).

* It does not tell us whether *neither* of the models provides a good fit.
  <div style="font-size: 70%; margin-top: -1em">
  In the JL paradox, with lots of data we prefer $\mathcal{H}\_0$ despite its bad fit, because $\mathcal{H}\_1$ is too diffuse.
  </div>

* Evidences (integrals) are hard to compute! (Nested sampling helps!)

<!-- .element style="margin-bottom: -1.5em" -->

The debate about how to overcome these limitations and/or find an alternative approach is far from settled (see refs. in sec. 1 of ref. above)
<!-- .element style="font-size: 85%; margins: 0 0 0 0;" -->


</section>

<section data-markdown>

## ALTERNATIVE: Mixture models

<div class="reference">
    <a href="https://arxiv.org/abs/1412.2044"> [*Testing hypotheses via a mixture estimation model*, Kamary et al '14]</a>
</div>

$$
\mathcal{M}\_0: \;
\theta_0 \in \Omega\_0\,,\;
\pi(\theta\_0\,|\,\mathcal{M}\_0)\,,\;
\mathcal{D} \sim \mathcal{L}\_0\left[\mathcal{D}\,|\,\\mathcal{M}\_0(\theta\_0)\right]
$$

$$
\mathcal{M}\_1: \;
\theta_1 \in \Omega\_1\,,\;
\pi(\theta\_1\,|\,\mathcal{M}\_1)\,,\;
\mathcal{D} \sim \mathcal{L}\_1\left[\mathcal{D}\,|\,\\mathcal{M}\_1(\theta\_1)\right]
$$

We define a *mixture model* $\mathcal{H}$:

$$
\mathcal{D} \sim (1-\alpha)\,\mathcal{L}\_1\left[\mathcal{D}\,|\,\\mathcal{M}\_1(\theta\_1)\right]
                   +\alpha \,\mathcal{L}\_0\left[\mathcal{D}\,|\,\\mathcal{M}\_0(\theta\_0)\right]
$$

$\mathcal{H}$ contains the two models for extreme values of the *mixture weight* $\alpha$.

<div style="font-size: 60%; margin-bottom:1em">
**NB:** unlike in classical mixture models, here all the data is assumed to have been generated by the same component, $\mathcal{M}\_0$ or $\mathcal{M}\_1$.
</div>

<div>
<img src="img/beta05.svg" alt="img" style="width:7em;float:left;"/>
<p>
We impose a reasonable prior $\alpha \sim \mathrm{Beta}(0.5,0.5)$, that concentrates the mass at the extremes (a priori, either is true, but not both).
</p>
<p>
Then, we draw samples directly from the *mixture model* $\mathcal{H}$, using **MCMC**, and we estimate simultaneously the parameter values and the weight $\alpha$ as a model selection index.
</p>
</div>

</section>

 <section data-markdown>

## ALTERNATIVE: Mixture models

<div class="reference">
    <a href="https://arxiv.org/abs/1412.2044"> [*Testing hypotheses via a mixture estimation model*, Kamary et al '14]</a>
</div>

### Advantages
<!-- .element style="margin-top:-.7em;" -->

* It can be proven that if one of the models actually generated the samples, $\alpha$ converges to the corresponding index.

* The posterior of $\alpha$ evaluates the strength of the support for each model in a natural way, together with the **uncertainty** on the decision.

* The sensitivity to the prior in $\alpha$ is easy to assess.

* Computational feasibility: MCMC's are easy to run and there is no need for evidence integration.

* It is extensible to more components, all explored at once.

* Improper priors are allowed... as long as they correspond to shared parameters.


</section>

<section data-markdown>

## ALTERNATIVE: Mixture models

<div class="reference">
    <a href="https://arxiv.org/abs/1412.2044"> [*Testing hypotheses via a mixture estimation model*, Kamary et al '14]</a>
</div>

### Example: JL!
<!-- .element style="margin-top:-.7em;" -->

$$
\mathcal{M}\_0: \; \pi(\mu\,|\,\mathcal{M}\_0) = \delta(0)\,,\;\;
\mathcal{M}\_1: \; \pi(\mu\,|\,\mathcal{M}\_1) = \mathcal{N}(0,1)
$$

$$
\mathcal{L}(\bar{x}\,|\,\mu) \sim \mathcal{N}(\mu,\sigma^2/N)
$$

![](img/mixtures.png)
<!-- .element style="width:70%; margin: 0 0 0 0" -->

<!-- .element style="text-align: center;" -->

</section>
